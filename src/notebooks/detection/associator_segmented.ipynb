{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T17:57:47.709446Z",
     "start_time": "2024-10-23T17:57:38.142679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import itertools\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import glob2\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from src.detection.associator_segmenter import best_matchups_combinatory, locate, find_candidates\n",
    "from src.detection.clustering import get_valid, get_clusters, get_delta\n",
    "from utils.data_reading.catalogs.ISC import ISC_file\n",
    "from utils.data_reading.sound_data.sound_file_manager import NpyFilesManager\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.bathymetry.bathymetry_grid import BathymetryGrid\n",
    "from utils.physics.sound.sound_model import HomogeneousSoundModel\n",
    "from utils.physics.sound.sound_velocity_grid import MonthlySoundVelocityGridOptimized\n",
    "from utils.transformations.features_extractor import STFTFeaturesExtractor"
   ],
   "id": "46fd83a0a52c0f95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 19:57:39.906925: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-23 19:57:39.969687: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-23 19:57:40.246960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-23 19:57:40.247073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-23 19:57:40.288114: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-23 19:57:40.376702: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-23 19:57:40.378410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-23 19:57:42.848805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/plerolland/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-23T18:36:56.180687Z",
     "start_time": "2024-10-23T18:36:55.930707Z"
    }
   },
   "source": [
    "datasets_yaml = \"/home/plerolland/Bureau/dataset.yaml\"\n",
    "\n",
    "year = 2012\n",
    "isc_file = f\"/home/plerolland/Bureau/catalogs/ISC/eqk_isc_revbull_{year}.txt\"\n",
    "detections_file = f\"../../data/detections/{year}/detections.npy\"  # obtained from associator_preprocess\n",
    "\n",
    "sound_model_h = HomogeneousSoundModel()\n",
    "sound_model_g = MonthlySoundVelocityGridOptimized([f\"../../data/sound_model/min-velocities_month-{i:02d}.nc\" for i in range(1,13)], interpolate=True)\n",
    "if 'bathy_model' not in locals():\n",
    "    bathy_model = BathymetryGrid.create_from_NetCDF(\"../../data/geo/GEBCO_2023_sub_ice_topo.nc\", lat_bounds=[-75, 35], lon_bounds=[-20, 180])\n",
    "\n",
    "stations_c = StationsCatalog(datasets_yaml).filter_out_undated().filter_out_unlocated()\n",
    "stations_c = stations_c.starts_before(datetime.datetime(year+1, 1, 1))\n",
    "stations_c = stations_c.ends_after(datetime.datetime(year, 1, 1))\n",
    "stft_computer = STFTFeaturesExtractor(None, vmin=60, vmax=140, f_min=5, f_max=60)\n",
    "\n",
    "MIN_P = 0.5\n",
    "MIN_P_MATES = 0.25\n",
    "MIN_P_MATES_tissnet = 0.3\n",
    "NB = 4\n",
    "TOLERANCE = datetime.timedelta(seconds=20)\n",
    "TIME_DELTA_SEARCH = datetime.timedelta(seconds=5*86400)\n",
    "\n",
    "TIME_RES = 0.5 # duration, in s, of a sample in the associator results\n",
    "HALF_FOCUS_SIZE = 16  # window to compare for associator -> keep what was used for training\n",
    "MAX_SHIFT = 128  # max allowed shift resulting from associator\n",
    "\n",
    "# output\n",
    "RES_FILE = f\"../../data/detections/{year}/matchups_clusters_loc_adjusted_experiment.csv\""
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detections computation",
   "id": "88224664cbbfd7d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T18:37:21.242674Z",
     "start_time": "2024-10-23T18:36:57.477791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detections = np.load(detections_file, allow_pickle=True).item()\n",
    "embedding_managers = {s : NpyFilesManager(f\"../../data/detections/{year}/embedding_{year}_{s.name}-{s.date_start.year}\", fill_empty_with_zeroes=False) for s in detections.keys()}\n",
    "\n",
    "for s in detections:\n",
    "    detections[s] = detections[s][np.argsort(detections[s][:,0])]\n",
    "    detections[s] = detections[s][detections[s][:,1]>MIN_P_MATES_tissnet]\n",
    "    detections[s] = detections[s][detections[s][:,2]>10]  # we only take events with STA>LTA\n",
    "\n",
    "# merge all detections and sort them by date\n",
    "stations = list(detections.keys())\n",
    "merged_detections = []\n",
    "for s, dets in detections.items():\n",
    "    for det in dets:\n",
    "        merged_detections.append((det[0], det[1], s))\n",
    "merged_detections = np.array(merged_detections, dtype=np.object_)\n",
    "merged_detections = merged_detections[np.argsort(merged_detections[:,0])]\n",
    "\n",
    "merged_detections_kept = merged_detections[merged_detections[:,1]>MIN_P]\n",
    "print(f\"{len(merged_detections_kept)} detections kept out of {len(merged_detections)}\")"
   ],
   "id": "60fd086cd7d6948",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186750 detections kept out of 613644\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ISC loading",
   "id": "38d01cead221f0ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T18:37:23.561839Z",
     "start_time": "2024-10-23T18:37:21.310811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "isc = ISC_file(isc_file)\n",
    "\n",
    "# filter isc events to keep only events from ridges\n",
    "to_del = set()\n",
    "for ID, event in isc.items.items():\n",
    "    if bathy_model.get_nearest_values(event.get_pos()) > 0:\n",
    "        to_del.add(ID)\n",
    "    lat, lon = tuple(isc[ID].get_pos())\n",
    "    if (lat>-5 and lon > 115) or (lat>-30 and lon > 130) or (lat>-50 and lon>150) or (lat>-20 and lon>85):\n",
    "        to_del.add(ID)\n",
    "for ID in to_del:\n",
    "    del isc.items[ID]\n",
    "print(f\"{len(to_del)} terrestrial events removed from catalog ({len(isc.items)} remain)\")\n",
    "\n",
    "IDs = list(isc.items.keys())\n",
    "\n",
    "# ISC clustering\n",
    "allowed_delta = 30\n",
    "delta = get_delta(isc, allowed_delta)\n",
    "valid = get_valid(allowed_delta, delta, IDs)\n",
    "clusters = get_clusters(IDs, valid)\n",
    "\n",
    "res = \"\"\n",
    "for id, cluster in clusters.items():\n",
    "    for ID in cluster:\n",
    "        res += f\"{isc[ID].get_pos()[0]},{isc[ID].get_pos()[1]},{id}\\n\"\n",
    "with open(\"res.csv\", \"w\") as f:\n",
    "    f.write(res)\n",
    "    \n",
    "date_min = [np.min([isc[ID].date for ID in cluster]) for cluster in clusters.values()]\n",
    "clusters = list(clusters.values())\n",
    "clusters = [clusters[i] for i in np.argsort(date_min)]\n",
    "\n",
    "# todo update d_h when changing months!"
   ],
   "id": "5cee0b364d4ef31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16681 terrestrial events removed from catalog (938 remain)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Association",
   "id": "52cfaf92c1f07786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T18:37:23.604342Z",
     "start_time": "2024-10-23T18:37:23.597087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_nearest(array,value):\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    diff = np.abs(value - array[idx])\n",
    "    diff_prev = np.abs(value - array[idx-1]) if idx>0 else None \n",
    "    if idx == len(array) or diff_prev is not None and diff_prev < diff:\n",
    "        return array[idx-1], diff_prev\n",
    "    else:\n",
    "        return idx, diff"
   ],
   "id": "30a787f98d4e73f1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-23T18:41:28.907300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_d_h_area(stations_c, pos, date_ref, date_min=None, date_max=None, bathy_thresh=400):\n",
    "    stations_crisis = stations_c\n",
    "    if date_min:\n",
    "        stations_crisis = stations_c.ends_after(date_min)\n",
    "    if date_max:\n",
    "        stations_crisis = stations_c.ends_after(date_max)\n",
    "    bathy = {s: bathy_model.get_along_path_nearest(pos, s.get_pos())[0] for s in stations_crisis}\n",
    "    stations_crisis = [s for s in stations_crisis if np.max(bathy[s])<bathy_thresh]\n",
    "    expected = {s: sound_model_g.get_sound_travel_time(pos, s.get_pos(), date=date_ref) for s in stations_crisis}\n",
    "    # d_h[s1][s2] = given a detection on s1, time to wait before getting it on s2 (can be negative)\n",
    "    d_h = {s1: {s2: datetime.timedelta(seconds=expected[s2] - expected[s1]) for s2 in stations_crisis} for s1 in\n",
    "           stations_crisis}\n",
    "    return d_h\n",
    "\n",
    "def compute_d_h_agnostic(stations_c, date_ref):\n",
    "    d_h = {s1: {s2:  sound_model_g.get_sound_travel_time(s1.get_pos(), s2.get_pos(), date=date_ref) if s1.get_pos()!=s2.get_pos() else 0 for s2 in stations_c} for s1 in\n",
    "           stations_c}\n",
    "    d_h = {s1: {s2: datetime.timedelta(seconds=d_h[s1][s2]) for s2 in stations_c} for s1 in stations_c}\n",
    "    return d_h\n",
    "\n",
    "def associate_cluster(cluster, stations_c, isc, merged_detections_kept, embedding_managers, res_file, nb=3, date_seen_delta=datetime.timedelta(seconds=20)):\n",
    "    centroid = np.mean([isc[ID].get_pos() for ID in cluster], axis=0)\n",
    "    dates = [isc[ID].date for ID in cluster]\n",
    "    date_min, date_max = np.min(dates)-datetime.timedelta(seconds=86400*1), np.max(dates)+datetime.timedelta(seconds=86400*1)\n",
    "\n",
    "    d_h = compute_d_h_area(stations_c, centroid, date_min, date_min, date_max)\n",
    "    associate_area(d_h, merged_detections_kept, embedding_managers, res_file, date_min, date_max, nb, date_seen_delta, initial_pos_loc=centroid, ref_name=cluster[0])\n",
    "\n",
    "def associate_area(d_h, merged_detections_kept, embedding_managers, res_file, date_min=None, date_max=None, nb=3, date_seen_delta=datetime.timedelta(seconds=20),\n",
    "              seen_dates=None, initial_pos_loc=None, ref_name=None, agnostic=False):\n",
    "    if seen_dates is None:\n",
    "        seen_dates = []\n",
    "    if len(d_h) <= nb:\n",
    "        return\n",
    "    idx_min = np.searchsorted(merged_detections_kept[:, 0], date_min, side='right') if date_min else 0\n",
    "    idx_max = np.searchsorted(merged_detections_kept[:, 0], date_max, side='left') if date_max else len(merged_detections_kept)-1\n",
    "    if len(d_h) == 0:\n",
    "        return\n",
    "    max_d = np.max([list(d.values()) for d in d_h.values()])\n",
    "    for idx in tqdm(range(idx_min, idx_max), position=0, leave=False):\n",
    "        detection = merged_detections_kept[idx]\n",
    "        \n",
    "        # update seen dates to keep only recent\n",
    "        to_del = list()\n",
    "        for i in range(len(seen_dates)):\n",
    "            if seen_dates[i] < detection[0] - max_d:\n",
    "                to_del.append(i)\n",
    "        seen_dates = list(np.delete(seen_dates, to_del, axis=0))\n",
    "        if len(seen_dates) > 0 and np.min(detection[0]-np.array(seen_dates)) < date_seen_delta:\n",
    "            continue\n",
    "            \n",
    "        associate(detection, d_h, embedding_managers, res_file, nb, date_seen_delta, seen_dates, initial_pos_loc=initial_pos_loc, ref_name=ref_name, ref_date=date_min, agnostic=agnostic)\n",
    "        \n",
    "        \n",
    "def associate(detection, d_h, embedding_managers, res_file, nb=3, date_seen_delta=datetime.timedelta(seconds=20), seen_dates=None, initial_pos_loc=None, ref_name=None, ref_date=None, agnostic=False):\n",
    "    if seen_dates is None:\n",
    "        seen_dates = []\n",
    "    candidates = find_candidates(detection, d_h, TOLERANCE, embedding_managers, min_p=MIN_P_MATES, agnostic=agnostic)\n",
    "    d1, s1 = detection[0], detection[-1]\n",
    "    \n",
    "    # remove already used dates and events undetected by TiSSNet\n",
    "    s2_to_del = list()\n",
    "    for s2 in candidates.keys():\n",
    "        to_del = list()\n",
    "        for j, candidate in enumerate(candidates[s2]):\n",
    "            if len(seen_dates) > 0 and np.min(candidate[0]-np.array(seen_dates)) < date_seen_delta:\n",
    "                to_del.append(j)\n",
    "            _, diff = find_nearest(detections[s2][:,0], candidate[0])\n",
    "            if diff > date_seen_delta:\n",
    "                to_del.append(j)\n",
    "        candidates[s2] = np.delete(candidates[s2], to_del, axis=0)\n",
    "        if len(candidates[s2]) == 0:\n",
    "            s2_to_del.append(s2)\n",
    "    for s2 in s2_to_del:\n",
    "        del candidates[s2]\n",
    "        \n",
    "    # check we have enough stations\n",
    "    if len(candidates) < nb:\n",
    "        return\n",
    "        \n",
    "    # get best matchups\n",
    "    scores = {s: c for s, c in candidates.items()}\n",
    "    best_matchups = best_matchups_combinatory(scores, nb, d_h, TOLERANCE, agnostic=agnostic)\n",
    "    if len(best_matchups) == 0:\n",
    "        return\n",
    "    matchup_scores = []\n",
    "    for matchup in best_matchups:\n",
    "        matchup = [[d1, 0, s1]] + [[d2, j2, s2] for (d2, j2, s2) in matchup]\n",
    "        loc_worked, loc_res = locate(matchup, sound_model_h, 10, initial_pos=initial_pos_loc)\n",
    "        matchup_scores.append(loc_res.cost if loc_res else np.inf)\n",
    "    best_matchup = best_matchups[np.argmin(matchup_scores)]\n",
    "\n",
    "    matchup = [[d1, 0, s1]] + [[d2, j2, s2] for (d2, j2, s2) in best_matchup]\n",
    "    loc_worked, loc_res = locate(matchup, sound_model_h, 10, initial_pos=initial_pos_loc)\n",
    "\n",
    "    # if it didn't work because loc was not close enough, we try deleting a station\n",
    "    if not loc_worked and type(loc_res) != list and len(matchup) > nb + 1:\n",
    "        to_del = np.argmax(loc_res.fun)  # index of maximum residual\n",
    "        matchup = matchup[:to_del] + matchup[to_del+1:]\n",
    "        loc_worked, loc_res = locate(matchup, sound_model_h, 10, initial_pos=initial_pos_loc)\n",
    "\n",
    "    # in case it worked, go further and try to locate\n",
    "    if loc_worked:\n",
    "        loc_worked, loc_res = locate(matchup, sound_model_g, 10, initial_pos=initial_pos_loc)\n",
    "        if loc_worked:\n",
    "            # add other stations\n",
    "            seen_stations = set([c[-1] for c in matchup])\n",
    "            for s, c in candidates.items():\n",
    "                if s in seen_stations:\n",
    "                    continue\n",
    "                c = np.array(c)\n",
    "                possible = [True] * len(c)\n",
    "                for (d2, _, s2) in matchup:\n",
    "                    min_d = d2 - d_h[s][s2] - TOLERANCE if agnostic else d2 + d_h[s][s2] - TOLERANCE\n",
    "                    possible = np.logical_and(possible, min_d < c[:,0])\n",
    "                    possible = np.logical_and(possible, c[:,0] < d2 + d_h[s2][s] + TOLERANCE)\n",
    "                if np.count_nonzero(possible) == 0:\n",
    "                    continue\n",
    "                c = c[possible]\n",
    "                for chosen in c[np.argsort(c[:,1])]:\n",
    "                    new_matchup = matchup + [chosen]\n",
    "                    loc_worked_new, loc_res_new = locate(new_matchup, sound_model_g, 10, initial_pos=initial_pos_loc)\n",
    "                    if loc_worked_new:\n",
    "                        seen_stations.add(s)\n",
    "                        loc_worked, loc_res = loc_worked_new, loc_res_new\n",
    "                        matchup = new_matchup\n",
    "                        break\n",
    "\n",
    "            det_times = [c[0] for c in matchup]\n",
    "            date_event = np.min(det_times) + datetime.timedelta(seconds=loc_res.x[0])\n",
    "            try:\n",
    "                J = loc_res.jac\n",
    "                cov = np.linalg.inv(J.T.dot(J))\n",
    "                var = np.sqrt(np.diagonal(cov))\n",
    "            except:\n",
    "                var = [-1, -1, -1]\n",
    "\n",
    "            # note: we register one ISC event from the cluster\n",
    "            to_write = (f'{date_event.strftime(\"%Y%m%d_%H%M%S\")},{loc_res.x[1]:.4f},{loc_res.x[2]:.4f},'\n",
    "                        f'{var[0]:.4f},{var[1]:.4f},{var[2]:.4f},{ref_name},{(date_event-ref_date).total_seconds() if ref_date else 0:.1f},{len(matchup)}')\n",
    "            for d, _, s in matchup:\n",
    "                to_write += f',{s.name}-{s.date_start.year},{d.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "                seen_dates.append(d)\n",
    "            with open(res_file, \"a\") as f:\n",
    "                f.write(to_write + \"\\n\")\n",
    "\n",
    "to_do = \"agnostic\"\n",
    "print(\"starting\")\n",
    "if to_do == \"clusters\":\n",
    "    first_cluster_idx = 0\n",
    "    if Path(RES_FILE).exists():\n",
    "        with open(RES_FILE, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        cluster_first_idxs = [c[0] for c in clusters]\n",
    "        first_cluster_idx = cluster_first_idxs.index(int(lines[-1].split(\",\")[6])) + 1\n",
    "    for i, cluster in tqdm(enumerate(clusters), total=len(clusters)):\n",
    "        if i < first_cluster_idx:\n",
    "            continue\n",
    "        associate_cluster(cluster, stations_c, isc, merged_detections_kept, embedding_managers, RES_FILE, NB)\n",
    "elif to_do == \"area\":\n",
    "    pos = -36.8, 77.4\n",
    "    d_h = compute_d_h_area(stations_c, pos, date_ref=datetime.datetime(2020,6,1))\n",
    "    res_file = RES_FILE[:-4]+\"_cluster_NAMS.csv\"\n",
    "    associate_area(d_h, merged_detections_kept, embedding_managers, res_file, initial_pos_loc=pos, date_min=merged_detections_kept[0,0])\n",
    "elif to_do == \"agnostic\":\n",
    "    d_h = compute_d_h_agnostic(stations_c, date_ref=datetime.datetime(2020,6,1))\n",
    "    res_file = RES_FILE[:-4]+\"_agnostic.csv\"\n",
    "    associate_area(d_h, merged_detections_kept, embedding_managers, res_file, date_min=merged_detections_kept[0,0], nb=NB, agnostic=True)"
   ],
   "id": "a70108be928ec377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Experiment",
   "id": "109527a38f5c3638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T17:04:31.882504Z",
     "start_time": "2024-10-21T17:03:58.268708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# testing code\n",
    "if True:\n",
    "    s1, s2 = stations_c.by_names(\"NEAMS\")[0], stations_c.by_names(\"WKER2\")[0]\n",
    "    print(\"STATIONS:\")\n",
    "    m1, m2 = embedding_managers[s1], embedding_managers[s2]\n",
    "    # 1.197507344E+09 2012 347005543.779   99.72\n",
    "    # 1.197507790E+09 2012 3470103 9.889  116.64\n",
    "    d1, d2 = datetime.datetime(2012, 12, 12, 3, 57, 40), datetime.datetime(2012, 12, 12, 4, 14, 0)\n",
    "    diff = get_embedder_similarities(d1, m1, d2, m2, min_p=None)\n",
    "    print(diff.shape)\n",
    "    plt.plot(np.arange(len(diff))-len(diff)//2, diff)\n",
    "    plt.xlim(-MAX_SHIFT, MAX_SHIFT)\n",
    "    plt.figure()\n",
    "    \n",
    "    delta = datetime.timedelta(seconds=MAX_SHIFT//2)\n",
    "    stft_computer.manager = s1.get_manager()\n",
    "    stft_computer.show_features(d1-delta, d1+delta)\n",
    "    \n",
    "    plt.figure()\n",
    "    stft_computer.manager = s2.get_manager()\n",
    "    stft_computer.show_features(d2-delta, d2+delta)\n",
    "    \n",
    "    p, h = get_embedder_similarities(d1, m1, d2, m2)\n",
    "    print(p,h)\n",
    "    \n",
    "# TODO make tolerance relative to distance (like 5%), updatable max bound of max-date when new dets are found, sort peaks by station distance"
   ],
   "id": "6b76a84ae44fa4d5",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# testing code\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m      3\u001B[0m     s1, s2 \u001B[38;5;241m=\u001B[39m stations_c\u001B[38;5;241m.\u001B[39mby_names(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNEAMS\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m0\u001B[39m], stations_c\u001B[38;5;241m.\u001B[39mby_names(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWKER2\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSTATIONS:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[16], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# testing code\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m      3\u001B[0m     s1, s2 \u001B[38;5;241m=\u001B[39m stations_c\u001B[38;5;241m.\u001B[39mby_names(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNEAMS\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m0\u001B[39m], stations_c\u001B[38;5;241m.\u001B[39mby_names(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWKER2\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSTATIONS:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/snap/pycharm-professional/418/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:755\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m/snap/pycharm-professional/418/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/418/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1220\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/418/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1234\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1235\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test = False\n",
    "if test:\n",
    "    centroid = np.mean([isc[ID].get_pos() for ID in cluster], axis=0)\n",
    "    dates = [isc[ID].date for ID in cluster]\n",
    "    date_min, date_max = np.min(dates), np.max(dates)+datetime.timedelta(seconds=86400)\n",
    "    date_mid = date_min + (date_max - date_min) / 2\n",
    "    \n",
    "    expected = {s: sound_model_g.get_sound_travel_time(centroid, s.get_pos(), date=date_mid) for s in stations}\n",
    "    # d_h[s1][s2] = given a detection on s1, time to wait before getting it on s2 (can be negative)\n",
    "    stations_crisis = stations_c.ends_after(date_min).starts_before(date_max)\n",
    "    bathy = {s: bathy_model.get_along_path_nearest(centroid, s.get_pos())[0] for s in stations_crisis}\n",
    "    stations_crisis = [s for s in bathy.keys() if np.max(bathy[s])<400 and s in detections.keys()]\n",
    "    d_h = {s1: {s2: datetime.timedelta(seconds=expected[s2] - expected[s1]) for s2 in stations_crisis} for s1 in\n",
    "           stations_crisis}\n",
    "    s = stations_c.by_names(\"WKER2\")[0]\n",
    "    d = (datetime.datetime(2012,12,12,4,11,12),s)\n",
    "    candidates = find_candidates(d, d_h, tolerance=TOLERANCE,\n",
    "                                         min_p=MIN_P_MATES)\n",
    "    scores = {s: c for s, c in candidates.items()}\n",
    "    best_m = best_matchups_combinatory(scores, NB, d_h, TOLERANCE)\n",
    "    print(best_m)\n",
    "    best_m = best_m[0]\n",
    "    matchup = [[d[0], 0, s]] + [[d, j, s] for (d, j, s) in best_m]\n",
    "    for c in matchup:\n",
    "        print(c[0], c[1], c[2].name)\n",
    "    \n",
    "    \n",
    "    # LOCATION TEST\n",
    "    det_times = [c[0] for c in matchup]\n",
    "    det_pos = [c[-1].get_pos() for c in matchup]\n",
    "    r = sound_model_g.localize_common_source(det_pos, det_times, initial_pos=list(centroid))\n",
    "    print(r)\n",
    "    \n",
    "    diff = get_embedder_similarities(matchup[1][0], embedding_managers[matchup[1][2]], d[0], embedding_managers[s], min_p=None)\n",
    "    print(diff.shape)\n",
    "    plt.plot(np.arange(len(diff))-len(diff)//2, diff)\n",
    "    plt.xlim(-MAX_SHIFT, MAX_SHIFT)"
   ],
   "id": "90ef184951bba534",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
